\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{float}
\usepackage{enumitem}

% Page layout
\geometry{
    left=2.5cm,
    right=2.5cm,
    top=3cm,
    bottom=3cm
}

% Header and footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Lab 6: Spark Climate Data Analysis}
\fancyhead[R]{Joaquim KELOGLANIAN}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

% Code listing style
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{bashstyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    frame=single
}

\lstset{style=bashstyle}

% Hyperlink colors
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=blue
}

% Title formatting
\titleformat{\section}{\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}

% Document starts
\begin{document}

% ============================================================================
% TITLE PAGE
% ============================================================================
\begin{titlepage}
    \centering
    \vspace*{2cm}
    
    {\LARGE \textbf{Institut Supérieur d'Électronique de Paris}} \\[0.5cm]
    {\Large ISEP} \\[2cm]
    
    {\Huge \textbf{Laboratory Report 6}} \\[0.5cm]
    {\LARGE Distributed Architectures and Programming} \\[0.3cm]
    {\large II3502} \\[3cm]
    
    {\huge \textbf{Spark Climate Data Analysis}} \\[0.5cm]
    {\Large Using Apache Spark RDDs for NOAA GSOD Data Processing} \\[4cm]
    
    \begin{tabular}{ll}
        \textbf{Student:} & Joaquim KELOGLANIAN \\[0.3cm]
        \textbf{Course:} & II3502 - Distributed Architectures and Programming \\[0.3cm]
        \textbf{Academic Year:} & 2025-2026 \\[0.3cm]
        \textbf{Date:} & December 16, 2025 \\[0.3cm]
    \end{tabular}
    
    \vfill
\end{titlepage}

% ============================================================================
% TABLE OF CONTENTS
% ============================================================================
\newpage
\tableofcontents
\newpage

% ============================================================================
% SECTION 1: INTRODUCTION
% ============================================================================
\section{Introduction}

This report documents a distributed climate data analysis application using Apache Spark's RDD API to analyze NOAA Global Surface Summary of the Day (GSOD) data. The application computes temperature trends, precipitation patterns, and extreme weather statistics using only RDD-based transformations (no DataFrames/SQL).

\textbf{Technology Stack:} Apache Spark 3.5.1, PySpark, Python 3.11, Docker, NOAA GSOD dataset.

% ============================================================================
% SECTION 2: APPLICATION ARCHITECTURE
% ============================================================================
\section{Application Architecture}

The application implements a six-stage Spark pipeline: Data Loading → Cleaning → Transformation → Aggregation → Analysis → Export.

\subsection{Key Processing Steps}

\textbf{1. Data Loading:} Windows-compatible file loading using Python's \texttt{glob} module, then parallelized with \texttt{sc.parallelize()}.

\textbf{2. Data Cleaning:} CSV parsing handles 28/29-field formats; filters invalid values (999.9, 9999.9); removes header row.

\textbf{3. Data Transformation:} Parse dates to extract year/month/season; decode FRSHTT flags for extreme events (Fog, Rain, Snow, Hail, Thunder, Tornado).

\begin{lstlisting}[language=Python, caption=Date and Season Parsing]
def parse_date(date_str):
    dt = datetime.strptime(date_str, "%Y-%m-%d")
    year, month = dt.year, dt.month
    if month in [12, 1, 2]: season = "Winter"
    elif month in [3, 4, 5]: season = "Spring"
    elif month in [6, 7, 8]: season = "Summer"
    else: season = "Autumn"
    return year, month, season
\end{lstlisting}

\textbf{4. Aggregations:} Compute monthly/yearly temperature averages, seasonal precipitation, top 10 max temperatures, extreme event counts, and summary statistics.

% ============================================================================
% SECTION 3: IMPLEMENTATION DETAILS
% ============================================================================
\section{Implementation Details}

\subsection{Core RDD Operations}

The application uses pure RDD API (no DataFrames/SQL). Key example showing monthly temperature aggregation:

\begin{lstlisting}[language=Python, caption=Monthly Average Temperature Aggregation]
monthly_avg_temp = (
    transformed_data
    .map(lambda r: ((r["station"], r["year"], r["month"]), 
                    (r["temp"], 1)))
    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))
    .mapValues(lambda v: v[0] / v[1])
)
\end{lstlisting}

\textbf{Extreme Event Detection:}
\begin{lstlisting}[language=Python, caption=FRSHTT Flag Parsing and Event Counting]
def parse_frshtt(frshtt_str):
    flags = ["Fog", "Rain", "Snow", "Hail", "Thunder", "Tornado"]
    return {flag: frshtt_str[i] == "1" 
            for i, flag in enumerate(flags)}

extreme_events = transformed_data.flatMap(
    lambda r: [((r["station"], event), 1) 
               for event in r["events"] if r["events"][event]]
).reduceByKey(lambda a, b: a + b)
\end{lstlisting}

\subsection{Data Quality and Optimization}

\textbf{CSV Format Handling:} Detects 28 vs 29-field formats; uses Python's \texttt{csv.reader} for proper quote handling.

\textbf{Missing Values:} Filters records where any numeric field exceeds 999 (catches both 999.9 and 9999.9).

\textbf{Performance:} Uses \texttt{local[*]} for all CPU cores; \texttt{coalesce(1)} for single output files; lazy evaluation minimizes unnecessary computations.

% ============================================================================
% SECTION 4: EXECUTION ENVIRONMENT
% ============================================================================
\section{Execution Environment}

\subsection{Spark Configuration}

\begin{lstlisting}[language=Python, caption=Spark Context Initialization]
conf = SparkConf() \
    .setAppName("ClimateDataAnalysis") \
    .setMaster("local[*]") \
    .set("spark.hadoop.fs.file.impl", 
         "org.apache.hadoop.fs.LocalFileSystem") \
    .set("spark.hadoop.fs.defaultFS", "file:///")
sc = SparkContext(conf=conf)
\end{lstlisting}

\subsection{Docker Container}

\textbf{Windows users must use Docker} due to Hadoop native library incompatibilities. The provided \texttt{run-docker-windows.sh} script automatically handles:
\begin{itemize}
    \item Creating \texttt{.dockerignore} to exclude \texttt{.venv/}
    \item Building image from Apache Spark 3.5.1 base
    \item Volume mounting for input/output
    \item Path conversion for Windows environments
\end{itemize}

\begin{lstlisting}[caption=Dockerfile (Condensed)]
FROM apache/spark:3.5.1
USER root
RUN pip install uv
USER spark
WORKDIR /app
COPY . .
RUN uv sync
CMD ["uv", "run", "python", "-m", "ii3502_lab6.climate_analysis"]
\end{lstlisting}

% ============================================================================
% SECTION 5: HOW TO RUN
% ============================================================================
\section{How to Run}

\subsection{Linux/macOS (Native)}

\begin{lstlisting}[language=bash, caption=Native Execution]
# Install dependencies
curl -LsSf https://astral.sh/uv/install.sh | sh
uv sync

# Run application
uv run python -m ii3502_lab6.climate_analysis
\end{lstlisting}

\subsection{Windows (Docker Required)}

\textbf{Windows MUST use Docker} due to Hadoop winutils incompatibilities.

\begin{lstlisting}[language=bash, caption=Docker Execution (Git Bash)]
# One-command execution (script auto-creates .dockerignore)
./run-docker-windows.sh
\end{lstlisting}

The script automatically: builds image with \texttt{--no-cache}, mounts \texttt{src/main/resources}, executes analysis, writes results to \texttt{src/main/resources/output/}.

\subsection{Output Structure}

Results are saved to \texttt{src/main/resources/output/} with 6 subdirectories: \texttt{monthly\_avg\_temp}, \texttt{yearly\_avg\_temp}, \texttt{seasonal\_prcp}, \texttt{highest\_max\_temp}, \texttt{extreme\_events}, \texttt{summary}. Each contains \texttt{\_SUCCESS} marker and \texttt{part-00000} data file (consolidated via \texttt{coalesce(1)}).

% ============================================================================
% SECTION 6: EXECUTION RESULTS
% ============================================================================
\section{Execution Results}

\subsection{Terminal Execution Screenshots}

This section presents terminal screenshots demonstrating the complete execution workflow of the Spark climate data analysis application.

\subsubsection{Application Startup and Initialization}

Figure \ref{fig:spark_init} shows the Spark context initialization with configuration details including master URL, application name, and default parallelism. This demonstrates the successful setup of the local Spark environment.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{screenshots/01_spark_initialization.png}
    \caption{Spark Context Initialization and Configuration}
    \label{fig:spark_init}
\end{figure}

\subsubsection{Data Loading and Cleaning}

Figure \ref{fig:data_loading} displays the data loading phase, showing the number of raw lines loaded from CSV files, header detection, and the data cleaning process. The output indicates how many valid records remain after filtering invalid values.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{screenshots/02_data_loading_cleaning.png}
    \caption{Data Loading, Header Detection, and Cleaning Process}
    \label{fig:data_loading}
\end{figure}

\subsubsection{Aggregations and Analysis}

Figure \ref{fig:aggregations} shows the execution of various aggregation operations including monthly averages, yearly averages, seasonal precipitation, and extreme event detection. The screenshot displays computation times for each operation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{screenshots/03_aggregations_analysis.png}
    \caption{Climate Metric Aggregations and Analysis Operations}
    \label{fig:aggregations}
\end{figure}

\subsubsection{Results Saving and Completion}

Figure \ref{fig:results_saving} demonstrates the final stage where computed results are saved to output directories. The screenshot shows the creation of output files for each metric type and the successful completion message.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{screenshots/04_results_saving_completion.png}
    \caption{Results Export and Application Completion}
    \label{fig:results_saving}
\end{figure}

\subsubsection{Output Files Verification}

Figure \ref{fig:output_verification} shows the verification of generated output files using terminal commands to list directory contents and display sample results from each output category.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{screenshots/05_output_verification.png}
    \caption{Output Directory Structure and Sample Results}
    \label{fig:output_verification}
\end{figure}

\subsection{Performance and Results}

\textbf{Performance:} Data loading ~1s, cleaning/validation <1s, aggregations <1s, output writing <2s. Total pipeline: 5-10 seconds for 3 stations (1095 records).

\textbf{Sample Results:} Hottest year 2025 (34.39°C avg), wettest station 01001499999 (4399.56mm total), highest gust 01001099999 (58.50 knots). Generated 6 output categories with monthly/yearly temperatures, seasonal precipitation, extreme event counts.

% ============================================================================
% SECTION 7: OBSERVATIONS
% ============================================================================
\section{Observations}

\textbf{Data Quality:} GSOD dataset has CSV format inconsistencies (28/29 fields), missing value indicators (999.9, 9999.9), and incomplete variables. Application handles these robustly.

\textbf{Spark Characteristics:} Lazy evaluation defers execution until actions; \texttt{local[*]} efficiently uses all cores; \texttt{coalesce(1)} simplifies output; RDD map-reduce patterns work well for aggregations.

\textbf{Docker Benefits:} Critical for Windows (eliminates Hadoop winutils issues); ensures consistent environment; enables reproducible results across platforms.

% ============================================================================
% SECTION 8: CHALLENGES
% ============================================================================
\section{Challenges and Solutions}

\textbf{1. Windows Compatibility:} PySpark requires Hadoop winutils on Windows, causing "Python worker crashed" errors. \textit{Solution:} Dual-mode loading (Python \texttt{glob} + \texttt{sc.parallelize()} for local files); Docker mandatory for Windows with automated \texttt{run-docker-windows.sh} script.

\textbf{2. CSV Format:} Inconsistent field counts (28/29) due to commas in station names. \textit{Solution:} Python \texttt{csv.reader} handles quoted fields; format detection adjusts field indexing.

\textbf{3. Missing Values:} Multiple indicators (999.9, 9999.9). \textit{Solution:} Validation function filters records where any field exceeds 999.

\textbf{4. Output Proliferation:} Spark creates multiple part files. \textit{Solution:} \texttt{coalesce(1)} consolidates to single file per metric.

% ============================================================================
% SECTION 9: CONCLUSIONS
% ============================================================================
\section{Conclusions}

This laboratory successfully demonstrated Apache Spark's RDD API for distributed climate data analysis. The application processes NOAA GSOD data using pure RDD operations (no DataFrames/SQL), handling real-world data quality issues and achieving cross-platform compatibility via Docker.

\textbf{Key Achievements:} Complete RDD pipeline with map-reduce patterns; robust CSV parsing; Windows compatibility via Docker automation; 6 climate metric categories computed efficiently.

\textbf{Learning Outcomes:} RDD transformations and actions; distributed aggregation patterns; containerization for reproducibility; climate data quality handling.

\textbf{Future Work:} Multi-year trend analysis; geographic aggregation with station metadata; cluster deployment (YARN/Kubernetes); DataFrame comparison; visualization dashboard.

% ============================================================================
% REFERENCES
% ============================================================================
\newpage
\section*{References}
\addcontentsline{toc}{section}{References}

\begin{enumerate}
    \item Apache Spark Documentation. \textit{RDD Programming Guide}. \\
          \url{https://spark.apache.org/docs/latest/rdd-programming-guide.html}
    
    \item Apache Spark Documentation. \textit{PySpark API Reference}. \\
          \url{https://spark.apache.org/docs/latest/api/python/}
    
    \item NOAA National Centers for Environmental Information. \textit{Global Surface Summary of the Day}. \\
          \url{https://www.ncei.noaa.gov/data/global-summary-of-the-day/}
    
    \item Docker Documentation. \textit{Docker Desktop for Windows}. \\
          \url{https://docs.docker.com/desktop/windows/}
    
    \item Keloglanian, J. (2025). \textit{II3502 Lab 6: Spark Climate Data Analysis}. \\
          GitHub Repository: \url{https://github.com/Joaquim-Keloglanian/II3502_Lab6}
\end{enumerate}

% ============================================================================
% APPENDIX: KEY CODE SNIPPETS
% ============================================================================
\newpage
\appendix
\section{Key Code Snippets}

\subsection{Windows-Compatible Data Loading}

\begin{lstlisting}[language=Python, caption=Python-based File Loading]
local_matches = glob.glob(input_path)
if local_matches:
    all_lines = []
    for file_path in local_matches:
        with open(file_path, 'r', encoding='utf-8') as f:
            all_lines.extend([line.rstrip() for line in f.readlines()])
    raw_data = sc.parallelize(all_lines)
else:
    raw_data = sc.textFile(input_path)  # Fallback
\end{lstlisting}

\subsection{CSV Field Extraction with Format Detection}

\begin{lstlisting}[language=Python, caption=Handling 28 vs 29 Field Formats]
def extract_fields(fields):
    if len(fields) == 29:  # Comma in station name
        return {"STATION": fields[0], "DATE": fields[1],
                "TEMP": fields[7], "MAX": fields[21], ...}
    elif len(fields) == 28:  # Standard format
        return {"STATION": fields[0], "DATE": fields[1],
                "TEMP": fields[6], "MAX": fields[20], ...}
\end{lstlisting}

\end{document}
